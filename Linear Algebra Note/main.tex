\documentclass[colorlinks]{tufte-handout}
% \usepackage[osf,sc]{mathpazo}
% \usepackage[scaled=0.90]{helvet}
% \usepackage[scaled=0.85]{beramono}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{bm}
\usepackage{pgf,tikz}
\usepackage{enumerate,enumitem}
\usepackage{sectsty}


% ---------- Preamble ----------
\pagestyle{fancyplain} % Make all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\allowdisplaybreaks
\geometry{
	left=13mm,
	textwidth=130mm,
	marginparsep=8mm,
	marginparwidth=55mm,
}
\setlist[enumerate]{
	label=(\alph*),
	leftmargin=2.5em,
	topsep=0.5em,
	itemsep=0em,
}
\allsectionsfont{\normalfont\bfseries}


% ---------- Theorem Environments ----------
\theoremstyle{plain} % default
\newtheorem{thm}{Theorem}
\newtheorem{thms}[thm]{Theorems}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{props}[thm]{Propositions}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}

\def\idea{\textit{\color[rgb]{0,0,.55}Proof Idea. }}

\newcounter{peq} % Set a new counter for problem-equations
\counterwithin{peq}{thm}
\newenvironment{peq}{%
   \equation
   \refstepcounter{peq}
%    \tag{P.~\thepeq}
   \tag{\thepeq}
}{%
   \endequation
}


% ---------- Symbol Macros ----------
\newcommand{\bra}[1]{\mathopen{}\left(#1\right)}
\newcommand{\sbra}[1]{\mathopen{}\left[#1\right]}
\newcommand{\cbra}[1]{\mathopen{}\left\{#1\right\}}
\newcommand{\norm}[1]{\mathopen{}\left\lVert#1\right\rVert}
\newcommand{\inp}[2]{\mathopen{}\left\langle#1,#2\right\rangle}
\newcommand{\abs}[1]{\mathopen{}\left|#1\right|}
\newcommand{\rest}[2]{\mathopen{}\left.#1\right|_{#2}}

\renewcommand{\phi}{\varphi}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\zero}{\mathbf{0}}

\renewcommand{\intercal}{t}
\newcommand{\e}{\mathrm{e}}

\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\im}{im}
% \DeclareMathOperator{\ker}{ker} % already existed
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}


% ---------- Title Section ----------
\title{	
	\normalfont\normalsize 
	{\itshape Linear Algebra Done Right} \\ [0pt]
	\huge Notes -- Linear Algebra
}
\author{Zhijie Chen}
\date{\vspace{-5pt}\normalsize\today}


\begin{document}
\justifying
\maketitle
\tableofcontents
\newpage

\thispagestyle{empty}
\begin{fullwidth}
	\Large
	\setstretch{1.5}
	\vspace*{\fill}
	\begin{center}
		Don't just read it; fight it!\\
		Ask your own questions.\\
		Look for your own examples.\\
		Discover your own proofs.\\
		Is the hypothesis necessary?\\
		Is the converse true?\\
		What happens in the classical special case?\\
		What about the degenerate cases?\\
		Where does the proof use the hypothesis?
	\end{center}
	\begin{flushright}
		\textemdash\,Paul Holmos\phantom{placeholderplaceholder}
	\end{flushright}
	\vspace*{\fill}
\end{fullwidth}
\newpage


% \section{Prerequisites}
% We declare several notations below for convenience and clarity.
% \begin{notns}[Basic notations]
% 	\phantom{linebreak}

%     \begin{itemize}
%         \item $\F$ denotes a number field.
%         \item $U$, $V$, $W$ denotes vector spaces (usually over scalar field $\F$).
%         \item $V^S$ denotes the set of functions from a nonempty set $S$ to a vector space $V$.
%     \end{itemize}
% \end{notns}


\section{Vector Spaces}
\begin{lem}[Linear dependence lemma]
	Suppose $v_1,\dots,v_m$ is a linearly dependent set in $V$. Then there exists $k\in\{1,2,\dots,m\}$ such that
	\[v_k\in\spn(v_1,\dots,v_m).\]
	Furthermore, removing the $k^\text{th}$ term from the list does not change the span.
\end{lem}

\begin{thm}
	Any two bases of a finite-dimensional vector space have the same length.%
	\footnote{This proposition ensures that the \emph{dimension} of a vector space is well-defined.}
\end{thm}
% \begin{proof}
% 	Suppose $V$ is finite-dimensional. Let $\mathcal{B}_1$ and $\mathcal{B}_2$ be two bases of $V$. Considering $\mathcal{B}_1$ as an independent set and $\mathcal{B}_2$ as a spanning set leads to $\#\mathcal{B}_1\leq\#\mathcal{B}_2$. Interchanging the roles of $\mathcal{B}_1$ and $\mathcal{B}_2$ and we have $\#\mathcal{B}_2\leq\#\mathcal{B}_1$. Thus $\#\mathcal{B}_1=\#\mathcal{B}_2$.
% \end{proof}


\section{Linear Maps}
\subsection{Kernal and Image of Linear Maps}
\begin{exer}
	Suppose $U$ and $V$ are finite-dimensional and $S\in\L\bra{V,W}$ and $T\in\L\bra{U,V}$. Prove that
	\[\dim\ker ST \leq \dim\ker S + \dim\ker T.\]
\end{exer}
\begin{proof}
	Restrict to $Z=\ker ST$. By the fundamental theorem of linear maps,
	\begin{align*}
		\dim Z & = \dim T(Z) + \dim\ker \rest{T}{Z} \\
		& \leq \dim T(Z) + \dim\ker T \\
		& = \dim ST(Z) + \dim\ker \rest{S}{T(Z)} + \dim\ker T \\
		& \leq \dim\ker S + \dim\ker T.\qedhere
	\end{align*}
\end{proof}

\begin{cor}[Sylvester's rank inequality]
	Suppose $A\in\F^{m,n}$ and $B\in\F^{n,p}$ are two matrices. Then%
	% \footnote{There is a slicker proof for this inequality using block matrices. But the proof here using linear maps is more informative.}
	\[\rank A+\rank B-n\leq\rank\bra{AB}.\]
\end{cor}


\subsection{Products and Quotients of Vector Spaces}
% \begin{lem}\label{lem: direct sum and product space}
% 	Suppose $V_1,\dots,V_m$ are subspaces of $V$.%
%     \footnote{Note that $V$ does not have to be finite-dimensional. Recall that $V_1+\cdots+V_m$ is a direct sum if and only if the only way to write $0$ as a sum of $v_1+\cdots+v_m$, where each $v_k\in V_k$, is by taking each $v_k$ equal to $0$.}
% 	Define a linear map $\Gamma: V_1\times\cdots\times V_m \to V_1+\cdots+V_m$ by
% 	\[\Gamma\bra{v_1,\dots,v_m} = v_1+\cdots+v_m.\]
% 	Then $V_1+\cdots+V_m$ is a direct sum if and only if $\Gamma$ is injective.
% \end{lem}

% \begin{thm}
% 	Suppose $V$ is finite-dimensional and $V_1,\dots,V_m$ are subspaces of $V$. Then $V_1+\cdots+V_m$ is a direct sum if and only if%
%     \footnote{Recall Lemma~\ref{lem: direct sum and product space}.}
% 	\[\dim\bra{V_1+\cdots+V_m} = \dim V_1+\cdots+\dim V_m.\]
% \end{thm}
% \begin{proof}
% 	Recall Lemma~\ref{lem: direct sum and product space}.Because $\Gamma$ is surjective, by the fundamental theorem of linear maps, $V_1+\cdots+V_m$ is a direct sum if and only if $\dim\bra{V_1+\cdots+V_m} = \dim\bra{V_1\times\cdots\times V_m} = \dim V_1+\cdots+\dim V_m$.
% \end{proof}

% \begin{notn}
% 	Suppose $T\in\L\bra{V,W}$. Define $\widetilde{T}: V+\ker V \to V$ by
% 	\[\widetilde{T}\bra{v+\ker T}=Tv.\]
% \end{notn}

\begin{exer}
	Suppose $V_1,\dots,V_m$ are vector spaces. Prove that $\L\bra{V_1\times\cdots\times V_m,W}$ and $\L\bra{V_1,W}\times\cdots\times\L\bra{V_m,W}$ are isomorphic vector spaces. 
\end{exer}
\begin{proof}
	We construct an isomorphism $T$ between the two vector spaces.
	
	For every $\Gamma\in\L\bra{V_1\times\cdots\times V_m,W}$, define $\phi_k:V_k\to W$ for each $k$ by
	\[\phi_k\bra{v_k}=\Gamma\bra{0,\dots,v_k,\dots,0}\]
	with $v_k$ in the $k^\text{th}$ slot and $0$ in all other slots. It can be verified that $\phi_k\in\L\bra{V_k,W}$.

	Define $T$ by $T\bra{\Gamma}=\bra{\phi_1,\dots,\phi_m}$. It can be verified that $T$ is a linear map. We prove $T$ is an isomorphism by constructing its inverse linear map $S$.

	For every $\bra{\phi_1,\dots,\phi_m}\in\L\bra{V_1,W}\times\cdots\times\L\bra{V_m,W}$, let
	\[S\bra{\phi_1,\dots,\phi_m}\bra{v_1,\dots,v_m}=\phi_1\bra{v_1}+\cdots+\phi_m\bra{v_m}.\]
	
	It can be shown that $S$ is a linear map, and that $S\circ T=I$ and $T\circ S=I$. That proves $T$ is indeed an isomorphism between the two vector spaces.
\end{proof}

\begin{prop}\label{prop: test for translate}
	A nonempty subset $A$ of $V$ is a translate of some subspace of $V$ if and only if $\lambda v+\bra{1-\lambda}w\in A$ for all $v,w\in A$ and all $\lambda\in\F$.
\end{prop}

\begin{exer}
	Suppose $A_1=v+U_1$ and $A_2=w+U_2$ for some $v,w\in V$ and some subspaces $U_1$, $U_2$ of $V$. Prove that $A_1\cap A_2$ is either the empty set or a translate of some subspace of $V$.%
    \footnote{Recall Proposition~\ref{prop: test for translate}.}
\end{exer}

\begin{prop}\label{prop: direct sum from quotient space}
	Suppose $U$ is a subspace of $V$ and $v_1+U,\dots,v_m+U$ is a basis of $V/U$ and $u_1,\dots,u_n$ is a basis of $U$. Then $v_1,\dots,v_m,u_1,\dots,u_n$ is a basis of $V$. In other words, $V=\spn(v_1,\dots,v_m)\oplus U$.%
    \footnote{$V=\spn(v_1,\dots,v_m)\oplus U$ still holds without the hypothesis that $U$ is finite-dimensional.}
\end{prop}

\begin{exer}
	Suppose $U$ is a subspace of $V$ such that $V/U$ is finite-dimensional.
	\begin{enumerate}
		\item Prove that if $W$ is a finite-dimensional subspace of $V$ and $V=U+W$, then $\dim W\geq\dim V/U$.
		\item Prove that there exists a finite-dimensional subspace $W$ of $V$ such that $V=U\oplus W$ and $\dim W=\dim V/U$.
	\end{enumerate}
\end{exer}
\begin{proof}
	Let $\overline{w}_1+U,\dots,\overline{w}_m+U$ be a basis of $V/U$. Then by Proposition~\ref{prop: direct sum from quotient space}, we have $V=\spn(\overline{w}_1,\dots,\overline{w}_m)\oplus U$. Let $W_0=\spn(\overline{w}_1,\dots,\overline{w}_m)$, then $V=U\oplus W_0$, as desired.

	Now we prove that for each subspace $W$ of $V$ such that $V=U+W$, we have $\dim W\geq m=\dim V/U$.
	
	For each $\overline{w}_i\in V$ above, by definition we have $\overline{w}_i=u_i+w_i$ for some $u_i\in U$ and $w_i\in W$. It can be shown from the linear independence of $\overline{w}_1+U,\dots,\overline{w}_m+U$ that $\overline{w}_1-u_1,\dots,\overline{w}_m-u_m$ are independent vectors in $W$. Hence $\dim W\geq m$.
\end{proof}


\subsection{Duality}
\begin{thm}
	Suppose $V$ and $W$ are finite-dimensional and $T\in\L\bra{V,W}$. Then
	\begin{center}
		$T$ is surjective $\iff T'$ is injective \quad and \quad $T$ is injective $\iff T'$ is surjective.%
		\footnote{This result can be useful because sometimes it is easier to verify that $T'$ is injective (or surjective) than to show directly that $T$ is surjective (or injective).}
	\end{center}
\end{thm}

\begin{prop}\label{prop: relation between subspace and annihilator}
	Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. Then
	\[U=\cbra{v\in V:\phi(v)=0\,\text{ for every }\,\phi\in U^0}.\]
\end{prop}

\begin{exer}
	Suppose $V$ is finite-dimensional and $U$ and $W$ are subspaces of $V$.
	\begin{enumerate}
		\item Prove that $W^0\subseteq U^0$ if and only if $U\subseteq W$.
		\item Prove that $W^0=U^0$ if and only if $U=W$.%
    	\footnote{Recall Proposition~\ref{prop: relation between subspace and annihilator}.}
	\end{enumerate}
\end{exer}

\begin{exer}
	Suppose $V$ is finite-dimensional and $U$ and $W$ are subspaces of $V$.\
	\begin{enumerate}
		\item Prove that $\bra{U+W}^0=U^0\cap W^0$.
		\item Prove that $\bra{U\cap W}^0=U^0+W^0$.
	\end{enumerate}
\end{exer}

\begin{prop}\label{prop: matrix of dual basis}
	Suppose $V$ is finite-dimensional and $v_1,\dots,v_n$ is a basis of $V$. Then $\phi_1,\dots,\phi_n\in V'$ is the dual basis of $v_1,\dots,v_n$ if and only if
	\[\begin{bmatrix}
		\M(\phi_1,(v_1,\dots,v_n))\\
		\vdots\\
		\M(\phi_n,(v_1,\dots,v_n))\\
	\end{bmatrix}=I.\]
	% where $\M\bra{\phi_i}$ is the $1\times n$ matrix of $\phi_i$ with respect to basis $v_1,\dots v_n$ of $V$ for each $i\in\cbra{1,\dots,n}$.
\end{prop}

\begin{exer}
	Suppose $V$ is finite-dimensional and $\phi_1,\dots,\phi_n$ is a basis of $V'$. Prove that there exists a basis of $V$ whose dual basis is precisely $\phi_1,\dots,\phi_n$.
\end{exer}
\begin{proof}
	We start from an arbitrary basis $u_1,\dots,u_n$ of $V$. Let $\psi_1,\dots,\psi_n$ be its dual basis. In this proof, we take standard basis $e_1,\dots,e_n$ as the basis of $\F^n$.
	
	Define $S,T\in\L\bra{V,\F^n}$ by
	\[T(v)=\bra{\phi_1(v),\dots,\phi_n(v)},\quad S(v)=\bra{\psi_1(v),\dots,\psi_n(v)}.\]
	Then by Proposition~\ref{prop: matrix of dual basis}, $\M\bra{S,\bra{u_1,\dots,u_n}}=I$.
	
	Let $A$ be the change of basis matrix from $\psi$'s to $\phi$'s, i.e.,
	% \[A=\M\bra{I,\bra{\phi_1,\dots,\phi_n},\bra{\psi_1,\dots,\psi_n}}.\]
	\[\begin{bmatrix}\phi_1&\cdots&\phi_n\end{bmatrix}=\begin{bmatrix}\psi_1&\cdots&\psi_n\end{bmatrix}A.\]
	Then by the definition of change of basis matrix, we have
	\begin{align*}
		\M\bra{T,\bra{u_1,\dots,u_n}}&=\begin{bmatrix}
			\M\bra{\phi_1,\bra{u_1,\dots,u_n}}\\
			\vdots\\
			\M\bra{\phi_n,\bra{u_1,\dots,u_n}}\\
		\end{bmatrix}
		=A^\intercal\begin{bmatrix}
			\M\bra{\psi_1,\bra{u_1,\dots,u_n}}\\
			\vdots\\
			\M\bra{\psi_n,\bra{u_1,\dots,u_n}}\\
		\end{bmatrix}\\\\
		&=A^\intercal\cdot\M\bra{S,\bra{u_1,\dots,u_n}}=A^\intercal.
	\end{align*}

	Consider basis $v_1,\dots,v_n$ of $V$ such that the change of basis matrix from $u$'s to $v$'s is $\bra{A^\intercal}^{-1}$.%
    \footnote{\idea The change of basis for $V'\to V'$ corresponds to the transpose of $V\leftarrow V$, where transpose and inverse both come from duality.}
	Thus
	\[\M\bra{T,\bra{v_1,\dots,v_n}}=\M\bra{T,\bra{u_1,\dots,u_n}}\cdot\M\bra{I,\bra{v_1,\dots,v_n},\bra{u_1,\dots,u_n}}=I.\]
	Then by Proposition~\ref{prop: matrix of dual basis}, the dual basis of $v_1,\dots,v_n$ is precisely $\phi_1,\dots,\phi_n$, as desired.
\end{proof}

\begin{exer}[A natural isomorphism from primal space onto double dual space]%
	\footnote{Suppose $V$ is finite-dimensional. Then $V$ and $V'$ are isomorphic, but finding an isomorphism from $V$ onto $V'$ generally requires choosing a basis of $V$. In contrast, the isomorphism $\Lambda$ from $V$ onto $V''$ does not require a choice of basis and thus is more natural.}%
	\footnote{Another natural isomorphism is $\pi'\in\L\bra{(V/U)',V'}$ where $\pi$ is the normal quotient map.}
	Define $\Lambda:V\to V''$ by
	\[(\Lambda v)(\phi)=\phi(v)\]
	for each $v\in V$ and $\phi\in V'$.
	\begin{enumerate}
		\item Prove that if $T\in\L(V)$, then $T''\circ\Lambda=\Lambda\circ T$.
		\item Prove that if $V$ is finite-dimensional, then $\Lambda$ is an isomorphism from $V$ onto $V''$.
	\end{enumerate}
\end{exer}


\section{Polynomials}
% \begin{thm}
% 	Suppose $p\in\P(\F)$ is a nonconstant polynomial of degree $m$. Then $\lambda\in\F$ is a zero of $p$ if and only if there exists a polynomial $q\in\P(\F)$ of degree $m-1$ such that $p(z)=(z-\lambda)q(z)$ for every $z\in\F$.
% \end{thm}

\begin{thm}
	Suppose $p\in\P(\F)$ is a nonconstant polynomial of degree $m$. Then $p$ has at most $m$ zeros in $\F$.%
	\footnote{This theorem implies that when a polynomial $p$ has too many zeros, $p=0$.}%
    \footnote{This theorem implies that the coefficients of a polynomial are uniquely determined. In particular, the \emph{degree} of a polynomial is well-defined.}
\end{thm}

\begin{thm}[Division algorithm for polynomials]
	Suppose that $p,s\in\P(\F)$, with $s\neq0$. Then there exist unique polynomials $q,r\in\P(\F)$ such that $p=sq+r$.%
	% \footnote{The division algorithm for polynomials can be proved without using any linear algebra. This proof makes a nice use of a basis of $\P_n(\F)$.}
\end{thm}
\begin{proof}
	Let $n=\deg p$ and $m=\deg s$. The case where $n<m$ is trivial. Thus we now assume that $n\geq m$.

	The list
	\[1,z,\dots,z^{m-1},s,zs,\dots,z^{n-m}s\]
	is linearly independent in $\P_n(\F)$. And it also has length $n+1$. Hence the list is a basis of $\P_n(\F)$.

	Because $p\in\P_n(\F)$, there exist unique constants $a_0,\dots,a_{m-1},b_0,\dots,b_{n-m}\in\F$ such that
	\begin{align*}
		p&=a_0+a_1z+\cdots+a_{m-1}z^{m-1}+b_0s+b_1zs+\cdots+b_{n-m}z^{n-m}s\\
		&=\bra{a_0+a_1z+\cdots+a_{m-1}z^{m-1}}+s\bra{b_0+b_1z+\cdots+b_{n-m}z^{n-m}}.\qedhere
	\end{align*}
\end{proof}

\begin{thm}[Fundamental theorem of algebra, first version]
	Every nonconstant polynomial with complex coefficients has a zero in $\C$.
\end{thm}
\begin{proof}
	Suppose $p\in\P(\C)$ is a nonconstant polynomial with highest-order nonzero term $c_mz^m$. Then $\abs{p(z)}\to\infty$ as $\abs{z}\to\infty$. Thus the continuous function $z\mapsto\abs{p(z)}$ has a global minimum at some $\zeta\in\C$. Assume that $p(\zeta)\neq0$.

	Consider polynomial $q(z)=p(z+\zeta)/p(\zeta)$. The function $z\mapsto\abs{q(z)}$ has a global minimum at $z=0$. Write
	\[q(z)=1+a_kz^k+\cdots+a_mz^m\]
	where $k$ is the smallest positive integer such that the coefficient of $z_k$ is nonzero.
	
	Let $\beta$ be a $k^\text{th}$ root of $-1/a_k$. There is a constant $c>1$ such that if $t\in(0,1)$, then
	\[\abs{q(t\beta)}\leq\abs{1+a_kt^k\beta^k}+ct^{k+1}=1-t^k(1-tc).\]
	Thus taking $t$ to be $1/(2c)$ leads to $\abs{q(t\beta)}<1$.%
	\footnote{\idea $z^k$ is the leading term in this infinitesimal. We hope its coefficient is negative real, and thus use de Moivre's theorem.}
	The contradiction implies that $p(\zeta)=0$, as desired.
\end{proof}

% \begin{thm}[Fundamental theorem of algebra, second version]
% 	If $p\in\P(\C)$ is a nonconstant polynomial, then $p$ has a unique factorization of the form
% 	\[p(z)=c(z-\lambda_1)\cdots(z-\lambda_m),\]
% 	where $c,\lambda_1,\dots,\lambda_m\in\C$.
% \end{thm}

% \begin{thm}[Factorization of a polynomial over $\R$]
% 	If $p\in\P(\R)$ is a nonconstant polynomial, then $p$ has a unique factorization of the form
% 	\[p(x)=c(x-\lambda_1)\cdots(x-\lambda_m)(x^2+b_1x+c_1)\cdots(x^2+b_Mx+c_M),\]
% 	where $m,M\in\N$ (possibly $0$) and $c,\lambda_1,\dots,\lambda_m,b_1,\dots,b_M,c_1,\dots,c_M\in\R$, with $b_k^2<4c_k$ for each $k$.
% \end{thm}

\begin{exer}
	Suppose $p,q\in\P(\C)$ are nonconstant polynomials with no zeros in common. Let $m=\deg p$ and $n=\deg q$. Prove that there exist $r\in\P_{n-1}(\C)$ and $s\in\P_{m-1}(\C)$ such that $rp+sq=1$.
\end{exer}
\begin{proof}
	Define $T:\P_{n-1}(\C)\times\P_{m-1}(\C)\to\P_{m+n-1}(\C)$ by $T(r,s)=rp+sq$. It can be shown that $T$ is an injective linear map. Because the domain space and target space have the same dimension, $T$ is surjective, completing the proof.
\end{proof}


\section{Eigenvalues and Eigenvectors}
\subsection{Invariant Subspaces}
% \begin{thm}
% 	Suppose $V$ is finite-dimensional, $T\in\L(V)$, and $\lambda\in\F$. Then the following are equivalent.%
% 	\footnote{The equivalences are useful in that they allow identifying an eigenvalue without explicitly constructing an eigenvector.}
% 	\begin{enumerate}
% 		\item $\lambda$ is an eigenvalue of $T$.
% 		\item $T-\lambda I$ is not injective.
% 		\item $T-\lambda I$ is not surjective.
% 		\item $T-\lambda I$ is not invertible.
% 	\end{enumerate}
% \end{thm}

% \begin{exer}
% 	Suppose $T\in\L(V)$ has no eigenvalues and $T^4=I$. Prove that $T^2=-I$.
% \end{exer}
% \begin{proof}
% 	$(T^2+I)(T+I)(T-I)=0$. Because $T$ has no eigenvalues, $T+I$ and $T-I$ are both invertible. Hence $T^2+I=0$.
% \end{proof}

\begin{exer}
	Suppose that $\lambda_1,\dots,\lambda_n\in\R$ are pairwise distinct. Prove that the list $\e^{\lambda_1x},\dots,\e^{\lambda_nx}$ is linearly independent in $\R^\R$.
\end{exer}
\begin{proof}
	Let $V=\spn(\e^{\lambda_1x},\dots,\e^{\lambda_nx})$.%
	\footnote{Alternatively we can let $V=D(\R)$.}
	Define $D\in\L(V)$ by $Df=f'$. Then $\e^{\lambda x}$ is an eigenvector of $D$ corresponding to $\lambda$. A list of eigenvectors corresponding to distinct eigenvalues is linearly independent.
\end{proof}

% \begin{defn}
% 	Suppose $V$ is finite-dimensional, $T\in\L(V)$, and $U$ is a subspace of $V$ invariant under $T$. The \emph{quotient operator} $T/U\in\L(V/U)$ is defined by
% 	\[(T/U)(v+U)=Tv+U\]
% 	% for each $v\in V$.
% \end{defn}

\begin{exer}
	Suppose $V$ is finite-dimensional, $T\in\L(V)$, and $U$ is a subspace of $V$ invariant under $T$. Prove that each eigenvalue of the quotient operator $T/U$ is an eigenvalue of $T$.
\end{exer}
\begin{proof}
	It suffices to show that $T/U-\lambda I=(T-\lambda I)/U$ is not injective $\implies$ $T-\lambda I$ is not injective. We prove that $T-\lambda I$ is invertible $\implies$ $(T-\lambda I)/U$ is injective.

	Suppose $T-\lambda I$ is invertible. $U$ being invariant under $T$ implies that $U$ is invariant under $T-\lambda I$. Thus $(T-\lambda I)v\in U\iff v\in U$. Suppose $((T-\lambda I)/U)(v+U)=0$. Then $(T-\lambda I)v\in U$, which implies that $v\in U$, i.e., $v+U=0$. That proves the injectivity of $(T-\lambda I)/U$.
\end{proof}

\begin{exer}
	Suppose $V$ is finite-dimensional and $T\in\L(V)$. Prove that $T$ has an eigenvalue if and only if there exists a subspace of $V$ of dimension $\dim V-1$ that is invariant under $T$.%
	\footnote{\idea Consider the zero entries in $\M(T)$ and its transpose matrix $\M(T')$.}
\end{exer}
\begin{proof}
	We first suppose that $T$ has an eigenvalue $\lambda$. Then $\lambda$ is an eigenvalue of $T'$. There exists $\phi\in V'$ such that $\phi\circ T=T'\phi=\lambda\phi$. Extend $\phi$ to a basis $\phi,\phi_2,\dots,\phi_n$ of $V'$ and let $v,v_2,\dots,v_n$ be the basis of $V$ whose dual basis is $\phi,\phi_2,\dots,\phi_n$. Then $(\phi\circ T)v_k=\lambda\phi(v_k)=0$ for every $k$. Because $\phi(T v_k)=0$ for every $k$, we have $Tv_k\in\spn(v_2,\dots,v_n)$. That proves that $\spn(v_2,\dots,v_n)$ is invariant under $T$.
	
	Reversing the steps above leads to an eigenvector of $T'$, completing the proof.
\end{proof}


\subsection{Minimal Polynomials}
\begin{exer}[Companion matrix of a polynomial]
	Suppose $a_0,\dots,a_{n-1}\in\F$. Let $T\in\L(\F^n)$ be such that $\M(T)$ (with respect to the standard basis) is
	\[\begin{bmatrix}
		0&&&&&-a_0\\
		1&0&&&&-a_1\\
		&1&0&&&-a_2\\
		&&\ddots&&&\vdots\\
		&&&&0&-a_{n-2}\\
		&&&&1&-a_{n-1}\\
	\end{bmatrix}\]
	Prove that the minimal polynomial of $T$ is the polynomial%
	\footnote{This exercise implies that every monic polynomial is the minimal polynomial of some operator. Hence an algorithm that could produce exact eigenvalues for each operators on each $\F^n$ does not exist.}
	\[a_0+a_1z+\cdots+a_{n-1}z^{n-1}+z^n.\]
\end{exer}

% \begin{prop}
% 	Suppose $T\in\L(V)$ and $p\in\P(\F)$. Then there exists a unique $r\in\P(\F)$ such that $p(T)=r(T)$ and $\deg r$ is less than the degree of the minimal polynomial of $T$.%
% 	\footnote{This proposition implies that every polynomial applied to an operator can be simplified to a polynomial of smaller degree.}
% \end{prop}

\begin{exer}
	Prove that every operator on a finite-dimensional vector space of dimension at least $2$ has an invariant subspace of dimension $2$.
\end{exer}
\begin{proof}
	Let $T\in\L(V)$ and $n=\dim V$. We use induction on $n$. The base case $n=2$ is trivial. Now suppose $n>2$ and the desired result holds for all smaller positive integers. Let $p$ be the minimal polynomial of $T$.

	If $T$ has an eigenvalue $\lambda$, then $p(z)=q(z)(z-\lambda)$ for some monic polynomial $q$ with $\deg q=\deg p-1$. Because $\rest{q(T)}{\im(T-\lambda I)}=0$, the desired result holds by induction hypothesis if $\dim\im(T-\lambda I)\geq2$. If $T-\lambda I=0$ the desired result trivially holds. If $\dim\im(T-\lambda I)=1$, then $(T-\lambda I)v$ is a scalar multiple of some fixed $u\in V$ for all $v\in V$. Take $w\in V\backslash\spn(u)$ and $\spn(u,w)$ will satisfy the desired property.

	If $T$ has no eigenvalues, then $\F=\R$ and $p(z)=q(z)(z^2+bz+c)$ for some $b,c\in\R$ with $b^2<4c$ and monic polynomial $q$ with $\deg q=\deg p-2$. If $\dim\im(T^2+bT+cI)\geq2$ the desired result holds by induction hypothesis. If $T^2+bT+cI=0$, then let $w\in V$ be such that $w\neq0$. It can be verified that $\spn(w,Tw)$ is invariant under $T$. If $\dim\im(T^2+bT+cI)=1$, because $\dim\ker(T^2+bT+cI)$ is even, $n$ is odd, which implies that $T$ has an eigenvalue. That completes the proof.
\end{proof}



\subsection{Commuting Operators}
\begin{exer}
	Suppose $\E\subseteq\L(V)$ and every element of $\E$ is diagonalizable. Prove that there exists a basis of $V$ with respect to which every element of $\E$ has a diagonal matrix if and only if every pair of elements of $\E$ commutes.%
	\footnote{This is an extension of simultaneous diagonalizability to more than $2$ (possibly infinitely many) operators.}
\end{exer}
\begin{proof}
	Suppose every pair of elements of $\E$ commutes. The other direction is trivial. We use induction on $n=\dim V$. The base case $n=1$ is trivial. Now suppose $n>1$ and the desired result holds for all smaller integers. Without loss of generality, suppose $\E\cap\{\lambda I:\lambda\in\F\}=\emptyset$, or else consider $\E\backslash\{\lambda I:\lambda\in\F\}$.

	Let $\lambda_1,\dots,\lambda_m$ be the distinct eigenvalues of $T\in\E$. Then $V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T)$. Because $E(\lambda_k,T)$ is invariant under every $S\in\E$, it suffices to show that the desired result holds on $E(\lambda_1,T)$. Because $E(\lambda_1,T)\subsetneqq V$, it holds by induction hypothesis, completing the proof.
\end{proof}

\begin{exer}
	Suppose $V$ is a finite-dimensional nonzero complex vector space. Suppose that $\E\subseteq\L(V)$ is such that $S$ and $T$ commute for all $S,T\in\E$.
	\begin{enumerate}
		\item Prove that there is a vector in $V$ that is an eigenvector for every element of $\E$.
		\item Prove that there exists a basis of $V$ with respect to which every element of $\E$ has an upper-triangular matrix.%
		\footnote{This is an extension of simultaneous upper triangularizability to more than $2$ (possibly infinitely many) operators.}
	\end{enumerate}
\end{exer}


\section{Inner Product Spaces}
\subsection{Inner Products and Norms}
\begin{thm}[Polarization identities]
	\begin{enumerate}
		\item Suppose $V$ is a real inner product space. Then
		\[\inp{u}{v}=\frac{\norm{u+v}^2-\norm{u-v}^2}{4}.\]
		\item Suppose $V$ is a complex inner product space. Then
		\[\inp{u}{v}=\frac{\norm{u+v}^2-\norm{u-v}^2+\norm{u+iv}^2i-\norm{u-iv}^2i}{4}.\]
	\end{enumerate}
	\label{thms: polarization identities}
\end{thm}

\begin{exer}\label{exer: condition for a norm-induced inner product}
	% A \emph{norm} on a vector space $U$ is a function
	% \[\norm{\cdot}:U\to[0,\infty)\]
	% which satisfies positive-definiteness, absolute homogeneity, and triangle inequality.

	Prove that if $\norm{\cdot}$ is a norm on $U$ satisfying the parallelogram equality, then there is an inner product $\inp{\cdot}{\cdot}$ on $U$ such that $\norm{u}=\inp{u}{u}^{1/2}$ for all $u\in U$.%
	\footnote{Recall Theorems~\ref{thms: polarization identities}.}
\end{exer}

\begin{exer}
	Suppose $f,g$ are differentiable functions from $\R$ to $\R^n$.
	\begin{enumerate}
		\item Prove that
		\[\inp{f(t)}{g(t)}'=\inp{f'(t)}{g(t)}+\inp{f(t)}{g'(t)}.\]
		\item Suppose $\norm{f(t)}\equiv c>0$ for every $t\in\R$. Prove that $\inp{f'(t)}{f(t)}=0$ for every $t\in\R$.%
		\footnote{This result has a nice geometric interpretation.}
	\end{enumerate}
\end{exer}


\subsection{Orthonormal Bases}
\begin{exer}
	Suppose $v_1,\dots,v_m$ is a linearly independent list in $V$. Prove that the orthonormal list produced by the formulas of the Gram-Schmidt procedure is the only orthonormal list $e_1,\dots,e_m$ in $V$ such that $\inp{v_k}{e_k}>0$ for each $k$.
\end{exer}

\begin{exer}
	Suppose $V$ is finite-dimensional. Suppose $\inp{\cdot}{\cdot}_1,\inp{\cdot}{\cdot}_2$ are inner products on $V$ with corresponding norms $\norm{\cdot}_1$ and $\norm{\cdot}_2$. Prove that there exists $b,c>0$ such that $b\norm{v}_2\leq\norm{v}_1\leq c\norm{v}_2$ for every $v\in V$.%
	\footnote{Note that \textit{Proof 1} does not use the hypothesis that $\norm{\cdot}_1$ and $\norm{\cdot}_2$ are associated with an inner product respectively.}
\end{exer}
\begin{proof}[Proof 1]
	Let $u_1,\dots,u_n$ be a basis of $V$. Without loss of generality, suppose that%
	\footnote{As an alternative to infinity norm, we might also use the $2$-norm, which naturally induces an inner product by Exercise~\ref{exer: condition for a norm-induced inner product}.}
	\[\norm{c_1u_1+\cdots+c_nu_n}_2=\max\{\abs{c_1},\dots,\abs{c_n}\}.\]
	We interpret $\norm{\cdot}_1$ as a function $(c_1,\dots,c_n)\mapsto\norm{c_1u_1+\cdots+c_nu_n}_1$ from $\F^n$ to $\R$. It suffices to show that the desired result holds on the sphere
	\[S=\cbra{(c_1,\dots,c_n)\in\F^n:\norm{c_1u_1+\cdots+c_nu_n}_2=1},\]
	i.e. that $\norm{\cdot}_1$ is bounded on $S$.%
	\footnote{\idea We cannot manipulate two norms simultaneously. Hence we restrict to $S$ and fix an orthonormal basis. Then $V$ behaves like $\F^n$. Nice properties of $S$ lead to proof of the continuity of $\norm{\cdot}_1$.}
	Because $S$ is a closed and bounded set, it suffices to show that the function $\norm{\cdot}_1$ is continuous on $\F^n$.
	
	Suppose $a\in\F^n$ and $e_1,\dots,e_n$ is the standard basis of $\F^n$. As $\norm{x-a}\to0$,
	\begin{align*}
		\abs{\norm{x}_1-\norm{a}_1}&\leq\norm{x-a}_1=\norm{\sum_{k=1}^{n}(x_k-a_k)e_k}_1\\
		&\leq\sum_{k=1}^{n}\abs{x_k-a_k}\norm{e_k}_1\\
		&\leq\bra{\sum_{k=1}^{n}\norm{e_k}_1^2}^{1/2}\norm{x-a}\to0,
	\end{align*}
	where the last inequality follows from the Cauchy-Schwarz inequality. Because every continuous function on a closed and bounded set is bounded, the proof is completed.
\end{proof}
\begin{proof}[Proof 2]
	Without loss of generality, suppose $V=\F^n$, $\norm{x}_1^2=x^\intercal Ax$, and $\norm{x}_2^2=x^\intercal Bx$, where $A,B$ are positive-definite and $B$ is diagonal.%
	\footnote{\idea Fixing an orthonormal basis naturally leads to a (simple) diagonal matrix.}
	Note that $\lambda B-A$ is Hermitian and hence diagonalizable. It suffices to show that there exists $\lambda>0$ such that $\lambda B-A$ is positive-definite. Such a $\lambda$ exists by the Gershgorin disk theorem, as desired.
\end{proof}

\begin{exer}
	Suppose $\F=\C$ and $V$ is finite-dimensional. Prove that if $T\in\L(V)$ is such that $1$ is the only eigenvalue of $T$ and $\norm{Tv}\leq\norm{v}$ for all $v\in V$, then $T$ is the identity operator.
\end{exer}
\begin{proof}
	By Schur's theorem, there exists an orthonormal basis $e_1,\dots,e_m$ of $V$ with respect to which $T$ has an upper-triangular matrix. Then all entries on the diagonal of $\M(T)$ is $1$. Any $\M(T)_{j,k}\neq0$ with $j\neq k$ would contradict $\norm{Te_k}\leq\norm{e_k}$. That proves that $\M(T)=I$, as desired.
\end{proof}

\begin{exer}
	Suppose $v_1,\dots,v_n$ is a basis of $V$. Prove that there exists a basis $u_1,\dots,u_n$ of $V$ such that%
	\footnote{\idea Linearity in inner product and and patterns of $0,1$ inspire the use of linear functionals.}
	\[\inp{u_j}{v_k}=\left\{\begin{aligned}
		0 \quad & \text{if}\, j\neq k,\\
		1 \quad & \text{if}\, j=k.\\
	\end{aligned}\right.\]
\end{exer}
\begin{proof}
	Define $\phi_k\in V'$ for each $k$ by $\phi_k(u)=\inp{u}{v_k}$. By the Riesz representation theorem, $\phi_1,\dots,\phi_n$ is a spanning list in $V'$. Thus it is a basis of $V'$. Let $u_1,\dots,u_n$ be the basis of $V$ whose dual basis is $\phi_1,\dots,\phi_n$. Then $u_1,\dots,u_n$ satisfies the desired property.
\end{proof}


\subsection{Orthogonal Complements and Minimization Problems}
\begin{thm}[Riesz representation theorem]
	Suppose $V$ is finite-dimensional. For each $v\in V$, define $\phi_v\in V'$ by
	\[\phi_v(u)=\inp{u}{v}\]
	for each $u\in V$. Then $v\mapsto\phi_v$ is a one-to-one map from $V$ onto $V'$.%
	\footnote{\idea If $\phi(u)=\inp{u}{v}$ holds for all $u\in V$, then $v\in(\ker\phi)^\perp$. However, $(\ker\phi)^\perp$ has dimension $1$ (except when $\phi=0$). Hence we can obtain the right $v$ by choosing an arbitrary nonzero $w\in(\ker\phi)^\perp$ and then multiplying by an appropriate scalar.}
\end{thm}
\begin{proof}
	The injectivity is trivial. We prove the surjectivity. Suppose $\phi\in V'$. The case where $\phi=0$ is trivial. Thus assume $\phi\neq0$. Hence $\ker\phi\neq V$, which implies that $(\ker\phi)^\perp\neq\{0\}$. Let $w\in(\ker\phi)^\perp$ be such that $w\neq0$. Let%
	\footnote{\idea Apply $w$ to $u$ to find the supposedly right scalar $c$ in $\phi(u)=\inp{u}{cw}$.}
	\begin{peq}\label{35peq: definition of $v$}
		v=\frac{\overline{\phi(w)}}{\norm{w}^2}w.
	\end{peq}
	Then $v\in(\ker\phi)^\perp$ and $v\neq0$. Now we prove that $\phi(u)=\inp{u}{v}$ for each $u\in V$.

	Let $u\in V$. The orthogonal decomposition leads to
	\[u=\frac{\inp{u}{v}}{\norm{v}^2}v+\bra{u-\frac{\inp{u}{v}}{\norm{v}^2}v},\]
	where the second term is orthogonal to $v$ and thus in $\ker\phi$. Applying $\phi$ to both sides, we have
	\begin{peq}\label{35peq: last equation}
		\phi(u)=\frac{\inp{u}{v}}{\norm{v}^2}\phi(v).
	\end{peq}
	By (\ref{35peq: definition of $v$}), we have
	\[\norm{v}=\frac{\abs{\phi(w)}}{\norm{w}} ,\qquad \phi(v)=\frac{\abs{\phi(w)}^2}{\norm{w}^2}.\]
	Applying them to (\ref{35peq: last equation}) leads to $\phi(u)=\inp{u}{v}$, as desired.
\end{proof}


\section{Complexifcation}
\begin{defn}
	The complexification of $V$, denoted by $V_\C$, equals $V\times V$ with normal addition and real scalar multiplication for product space. But we write an element $(u,v)$ of $V_\C$ as $u+iv$. Complex scalar multiplication is defined by
	\[(a+bi)(u+iv)=(au-bv)+i(av+bu)\]
	for all $a,b\in\R$ and all $u,v\in V$.%
	\footnote{Think of $V$ as a subset of $V_\C$ by identifying $u\in V$ with $u+i0$. The construction of $V_\C$ from $V$ can then be thought of as generalizing the construction of $\C^n$ from $\R^n$.}
\end{defn}

\begin{props}[Properties of complexification]
	\begin{enumerate}
		\item $\lambda\in\R$ is an eigenvalue of $T$ if and only if $\lambda$ is an eigenvalue of $T_\C$.
		\item $\lambda\in\C$ is an eigenvalue of $T_\C$ if and only if $\overline{\lambda}$ is an eigenvalue of $T_\C$.
		\item The minimal polynomial of $T_\C$ equals the minimal polynomial of $T$.
		\item Suppose $V$ is a real inner product space. For $u,v,w,x\in V$, define
		\[\inp{u+iv}{w+ix}_\C=\inp{u}{w}+\inp{v}{x}+(\inp{v}{w}-\inp{u}{x})i.\]
		Then $\inp{\cdot}{\cdot}_\C$ makes $V_\C$ into a complex inner product space. If $u,v\in V$, then
		\[\inp{u}{v}_\C=\inp{u}{v}\quad\text{and}\quad\norm{u+iv}_\C^2=\norm{u}^2+\norm{v}^2.\]
	\end{enumerate}
\end{props}

\begin{prop}
	Every operator on a finite-dimensional nonzero vector space has an invariant subspace of dimension $1$ or $2$.
\end{prop}
\begin{proof}
	The case where $\F=\C$ is trivial. Now assume $V$ is a real vector space and $T\in\L(V)$. Then $T_\C$%
	\footnote{\idea Field extension.}
	has an eigenvalue $a+bi$ with $a,b\in\R$. Thus there exist $u,v\in V$, not both $0$, such that
	\[Tu+iTv=(au-bv)+(av+bu)i.\]
	Hence $\spn(u,v)$ is invariant under $T$, as desired.
\end{proof}


\end{document}